{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","execution_count":4},{"cell_type":"code","source":"#Bert Tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\ndef preprocess_function(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answers\"]\n  #Tokenizer Configurations\n    inputs = tokenizer(\n        questions,\n        contexts,\n        max_length=384,\n        truncation=\"only_second\",  #Only Truncate the contexts\n        stride=128, #Splitting into overloaping chnuks\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\"\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = inputs.pop(\"offset_mapping\")\n\n    start_positions = []\n    end_positions = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = inputs[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id) #Qiestion start index\n\n        sequence_ids = inputs.sequence_ids(i)\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n\n        if len(answer[\"answer_start\"]) == 0: #when empty answer\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n        else:\n            start_char = answer[\"answer_start\"][0] #start of the answer\n            end_char = start_char + len(answer[\"text\"][0]) #end of the answer\n\n            # Find context token indices\n            context_start = sequence_ids.index(1)\n            context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n\n            start_pos = cls_index\n            end_pos = cls_index\n            for idx in range(context_start, context_end + 1):\n                if offsets[idx][0] <= start_char < offsets[idx][1]:\n                    start_pos = idx\n                if offsets[idx][0] < end_char <= offsets[idx][1]:\n                    end_pos = idx\n                    break\n\n            start_positions.append(start_pos)\n            end_positions.append(end_pos)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names) #applying preprocessing","metadata":{"id":"9Jmbty07FdF6","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f6f6431150744028a5f82d76f558e89f","5fe67cffb5364d5aa993381d88335bd6","1fd4b5bdcce64f449cad46962ac90423","0326203cbfa644e489249d2e3063c591","9ab83795464f47b095e8e96588be9cd6","1c15b270b03d4c02a030d84dbd905e22","ae5d0bb56e24417d9c3e785bcd778b0a","a6ad019a9e364714a6016aef53a4867d","b7bdbd5eb33b41dca2b5c36a396f4b62","c6c7ccb155254c7eb72dc271a7b5b76f","dec875e5548649f9ab3273b80351a4d2"]},"outputId":"3542ddf4-8026-42a5-b5d4-da473a640332","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:41:23.226066Z","iopub.execute_input":"2025-05-22T06:41:23.226436Z","iopub.status.idle":"2025-05-22T06:41:29.827270Z","shell.execute_reply.started":"2025-05-22T06:41:23.226405Z","shell.execute_reply":"2025-05-22T06:41:29.826657Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c53fb85f7394266a3eedfadd2e48a3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfa1c2e203c4ce8b8e97dedae36afac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c31b485d70945399385920c9564e201"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cebce61e48004c8195dcd3e88be3e7a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9592653b0db45fbb25b6595d630df9d"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"#data splitting\nsplit_dataset = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\ntrain_set = split_dataset['train']\nvalidatation_set = split_dataset['test']","metadata":{"id":"ypRLlg2tGnzq","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:41:29.828353Z","iopub.execute_input":"2025-05-22T06:41:29.828631Z","iopub.status.idle":"2025-05-22T06:41:29.842710Z","shell.execute_reply.started":"2025-05-22T06:41:29.828611Z","shell.execute_reply":"2025-05-22T06:41:29.842154Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Computing f1 score\ndef compute_metrics(p: EvalPrediction):\n    predictions, labels = p\n    start_preds, end_preds = predictions\n    start_labels, end_labels = labels\n\n    # Calculate F1 scores\n    start_f1 = f1_score(start_labels, np.argmax(start_preds, axis=1), average=\"macro\")\n    end_f1 = f1_score(end_labels, np.argmax(end_preds, axis=1), average=\"macro\")\n\n    return {\n        \"start_f1\": start_f1,\n        \"end_f1\": end_f1,\n        \"avg_f1\": (start_f1 + end_f1) / 2\n    }\n\n","metadata":{"id":"b07TG802G9ZC","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:41:29.843456Z","iopub.execute_input":"2025-05-22T06:41:29.843670Z","iopub.status.idle":"2025-05-22T06:41:29.876189Z","shell.execute_reply.started":"2025-05-22T06:41:29.843654Z","shell.execute_reply":"2025-05-22T06:41:29.875648Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\nargs = TrainingArguments(\n    output_dir=\"bert-finetuned-squad\", #Directory to save model checkpoints and logs.\n    run_name=\"bert-squad-run\",\n    eval_strategy =\"epoch\", #When to eval the model -> after each epoc\n    save_strategy =\"epoch\", #check point saved at each epoc\n    learning_rate=2e-5,\n    per_device_train_batch_size=8, #Number of samples assigned to each GPU during training\n    per_device_eval_batch_size=8, #same but during evalutaion\n    num_train_epochs=4, #Number of epocs\n    weight_decay=0.01, #Reglarization\n    push_to_hub=False,\n    logging_dir=\"./logs\",                      #  Required for logging\n    logging_strategy=\"steps\",                  #  Enable logging\n    logging_steps=100,                         #  Log every 100 steps\n    report_to=\"none\",                          #  Disable external logging tools like WandB\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_set,\n    eval_dataset=validatation_set,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"id":"_LWv_ByjHcAi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9424610a-4ca0-4d41-fb96-7478dc56e8eb","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:41:29.877552Z","iopub.execute_input":"2025-05-22T06:41:29.877761Z","iopub.status.idle":"2025-05-22T06:41:32.822761Z","shell.execute_reply.started":"2025-05-22T06:41:29.877745Z","shell.execute_reply":"2025-05-22T06:41:32.821952Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ef294e758b4d51b981c8c94f96b9cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3517019950.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"hWQ-Gf1sJwUi","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"3706138b-ff00-4a92-df20-1d2bdffea5ee","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:41:32.824032Z","iopub.execute_input":"2025-05-22T06:41:32.824443Z","iopub.status.idle":"2025-05-22T07:11:38.138201Z","shell.execute_reply.started":"2025-05-22T06:41:32.824416Z","shell.execute_reply":"2025-05-22T07:11:38.137669Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4056' max='4056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4056/4056 30:03, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Start F1</th>\n      <th>End F1</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.527000</td>\n      <td>1.385012</td>\n      <td>0.516836</td>\n      <td>0.546346</td>\n      <td>0.531591</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.035000</td>\n      <td>1.280725</td>\n      <td>0.559763</td>\n      <td>0.600943</td>\n      <td>0.580353</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.762400</td>\n      <td>1.325989</td>\n      <td>0.570978</td>\n      <td>0.606469</td>\n      <td>0.588723</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.478100</td>\n      <td>1.439299</td>\n      <td>0.574116</td>\n      <td>0.595774</td>\n      <td>0.584945</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4056, training_loss=1.066779023796849, metrics={'train_runtime': 1804.851, 'train_samples_per_second': 17.972, 'train_steps_per_second': 2.247, 'total_flos': 6356566201116672.0, 'train_loss': 1.066779023796849, 'epoch': 4.0})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"trainer.save_model(\"bert-finetuned-squad\")\ntokenizer.save_pretrained(\"bert-finetuned-squad\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_uNRIL7MWqh","outputId":"fb55c806-a71c-4078-adaa-37a7aa55a9e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('bert-finetuned-squad/tokenizer_config.json',\n"," 'bert-finetuned-squad/special_tokens_map.json',\n"," 'bert-finetuned-squad/vocab.txt',\n"," 'bert-finetuned-squad/added_tokens.json',\n"," 'bert-finetuned-squad/tokenizer.json')"]},"metadata":{},"execution_count":34}],"execution_count":34},{"cell_type":"code","source":"from transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\n# Sample test\ncontext = \"\"\"\nThe Great Wall of China is a series of fortifications made of stone, brick,\ntamped earth, wood, and other materials, generally built along an east-to-west line across\nthe historical northern borders of China to protect the Chinese states against invasions.\nSeveral walls were being built as early as the 7th century BC,\nbut the most famous sections were constructed by the Ming dynasty (1368–1644).\n\"\"\"\n\nquestions = [\n    \"Which Chinese dynasty built the most famous sections of the Great Wall?\",\n    \"What was the primary purpose of the Great Wall of China?\",\n    \"When was the wall build?\"\n           ]\nfor question in questions:\n    result = qa_pipeline(question=question, context=context)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {result['answer']}\")\n    print(f\"Score: {result['score']:.4f}\")","metadata":{"id":"DsZQbx7yN1hx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb646594-0645-45ed-fbca-be2de51a5598","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T07:11:38.139423Z","iopub.execute_input":"2025-05-22T07:11:38.139651Z","iopub.status.idle":"2025-05-22T07:11:38.176588Z","shell.execute_reply.started":"2025-05-22T07:11:38.139634Z","shell.execute_reply":"2025-05-22T07:11:38.176045Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Question: Which Chinese dynasty built the most famous sections of the Great Wall?\nAnswer: the Ming dynasty\nScore: 0.4801\nQuestion: What was the primary purpose of the Great Wall of China?\nAnswer: fortifications\nScore: 0.1651\nQuestion: When was the wall build?\nAnswer: 7th century BC\nScore: 0.7071\n","output_type":"stream"}],"execution_count":10}]}
