{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896},{"sourceId":11674705,"sourceType":"datasetVersion","datasetId":7327121}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport nltk\nimport sentencepiece\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense,Input,Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:16.818463Z","iopub.execute_input":"2025-05-04T21:04:16.819108Z","iopub.status.idle":"2025-05-04T21:04:16.823663Z","shell.execute_reply.started":"2025-05-04T21:04:16.819087Z","shell.execute_reply":"2025-05-04T21:04:16.822921Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Read the file line by line into a DataFrame\ndef read_file(where):\n    with open(where, \"r\", encoding=\"utf-8\") as f:\n        lines = f.read().splitlines()\n    return  pd.DataFrame(lines, columns=[\"sentence\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:35:05.851272Z","iopub.execute_input":"2025-05-04T19:35:05.851726Z","iopub.status.idle":"2025-05-04T19:35:05.855775Z","shell.execute_reply.started":"2025-05-04T19:35:05.851700Z","shell.execute_reply":"2025-05-04T19:35:05.855049Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#Data loading \nen_data = read_file(\"/kaggle/input/machine-translation/europarl-v7.fr-en.en\")\nprint(en_data.head())\nfr_data = read_file(\"/kaggle/input/machine-translation/europarl-v7.fr-en.fr\")\nprint(fr_data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:35:05.856468Z","iopub.execute_input":"2025-05-04T19:35:05.856738Z","iopub.status.idle":"2025-05-04T19:35:18.099205Z","shell.execute_reply.started":"2025-05-04T19:35:05.856715Z","shell.execute_reply":"2025-05-04T19:35:18.098528Z"}},"outputs":[{"name":"stdout","text":"                                            sentence\n0                          Resumption of the session\n1  I declare resumed the session of the European ...\n2  Although, as you will have seen, the dreaded '...\n3  You have requested a debate on this subject in...\n4  In the meantime, I should like to observe a mi...\n                                            sentence\n0                              Reprise de la session\n1  Je déclare reprise la session du Parlement eur...\n2  Comme vous avez pu le constater, le grand \"bog...\n3  Vous avez souhaité un débat à ce sujet dans le...\n4  En attendant, je souhaiterais, comme un certai...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#Just half the dataset\nsample_size = 500000\nen_data_sample = en_data[:sample_size]\nfr_data_sample = fr_data[:sample_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:35:18.101085Z","iopub.execute_input":"2025-05-04T19:35:18.101763Z","iopub.status.idle":"2025-05-04T19:35:18.105486Z","shell.execute_reply.started":"2025-05-04T19:35:18.101738Z","shell.execute_reply":"2025-05-04T19:35:18.104832Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#Exporting the english samples to train them into sentencepiece and get vocabulary with 8k in size \nen_data_sample.to_csv('en_sample.txt', index=False, header=False)\nsentencepiece.SentencePieceTrainer.train(input='en_sample.txt', model_prefix='bpe_en', vocab_size=10000,\n                                        control_symbols=['<start>', '<end>'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:35:18.106236Z","iopub.execute_input":"2025-05-04T19:35:18.106503Z","iopub.status.idle":"2025-05-04T19:36:26.302143Z","shell.execute_reply.started":"2025-05-04T19:35:18.106482Z","shell.execute_reply":"2025-05-04T19:36:26.301062Z"}},"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: en_sample.txt\n  input_format: \n  model_prefix: bpe_en\n  model_type: UNIGRAM\n  vocab_size: 10000\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  control_symbols: <start>\n  control_symbols: <end>\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: en_sample.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 500000 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <start>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <end>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=76854470\ntrainer_interface.cc(550) LOG(INFO) Done: 99.9535% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=71\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=0.999535\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 500000 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=41431625\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 182916 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 500000\ntrainer_interface.cc(609) LOG(INFO) Done! 160450\nunigram_model_trainer.cc(602) LOG(INFO) Using 160450 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=69797 obj=10.244 num_tokens=358411 num_tokens/piece=5.13505\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=55610 obj=7.71305 num_tokens=359626 num_tokens/piece=6.46693\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=41698 obj=7.67586 num_tokens=376022 num_tokens/piece=9.01775\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=41683 obj=7.67148 num_tokens=376258 num_tokens/piece=9.02665\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=31261 obj=7.69484 num_tokens=403882 num_tokens/piece=12.9197\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=31261 obj=7.69092 num_tokens=403857 num_tokens/piece=12.9189\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=23445 obj=7.72743 num_tokens=439218 num_tokens/piece=18.734\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=23445 obj=7.72083 num_tokens=439219 num_tokens/piece=18.734\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17583 obj=7.78388 num_tokens=477913 num_tokens/piece=27.1804\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17583 obj=7.77401 num_tokens=477905 num_tokens/piece=27.1799\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=13187 obj=7.86644 num_tokens=518855 num_tokens/piece=39.3459\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=13187 obj=7.85156 num_tokens=518831 num_tokens/piece=39.3441\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11000 obj=7.9261 num_tokens=544090 num_tokens/piece=49.4627\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11000 obj=7.91473 num_tokens=544093 num_tokens/piece=49.463\ntrainer_interface.cc(687) LOG(INFO) Saving model: bpe_en.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe_en.vocab\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"fr_data_sample.to_csv('fr_sample.txt', index=False, header=False)\nsentencepiece.SentencePieceTrainer.train(input='fr_sample.txt', model_prefix='bpe_fr', vocab_size=10000,\n                                         control_symbols=['<start>', '<end>'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:36:26.303931Z","iopub.execute_input":"2025-05-04T19:36:26.304181Z","iopub.status.idle":"2025-05-04T19:37:45.755219Z","shell.execute_reply.started":"2025-05-04T19:36:26.304156Z","shell.execute_reply":"2025-05-04T19:37:45.754441Z"}},"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: fr_sample.txt\n  input_format: \n  model_prefix: bpe_fr\n  model_type: UNIGRAM\n  vocab_size: 10000\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  control_symbols: <start>\n  control_symbols: <end>\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: fr_sample.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 500000 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <start>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <end>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=84886040\ntrainer_interface.cc(550) LOG(INFO) Done: 99.9575% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=82\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=0.999575\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 500000 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=47649269\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 248516 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 500000\ntrainer_interface.cc(609) LOG(INFO) Done! 203799\nunigram_model_trainer.cc(602) LOG(INFO) Using 203799 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=89273 obj=11.3481 num_tokens=482145 num_tokens/piece=5.40079\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=71947 obj=8.5525 num_tokens=482154 num_tokens/piece=6.70152\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=53956 obj=8.52638 num_tokens=503948 num_tokens/piece=9.33998\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=53944 obj=8.52065 num_tokens=503902 num_tokens/piece=9.34121\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40457 obj=8.55034 num_tokens=540885 num_tokens/piece=13.3694\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40457 obj=8.54439 num_tokens=540842 num_tokens/piece=13.3683\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30342 obj=8.5932 num_tokens=585282 num_tokens/piece=19.2895\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30342 obj=8.58475 num_tokens=585245 num_tokens/piece=19.2883\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22756 obj=8.65928 num_tokens=633165 num_tokens/piece=27.8241\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22756 obj=8.64923 num_tokens=633154 num_tokens/piece=27.8236\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17067 obj=8.75297 num_tokens=682515 num_tokens/piece=39.9903\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17067 obj=8.73738 num_tokens=682515 num_tokens/piece=39.9903\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12800 obj=8.88257 num_tokens=732493 num_tokens/piece=57.226\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12800 obj=8.85653 num_tokens=732447 num_tokens/piece=57.2224\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11000 obj=8.9454 num_tokens=757454 num_tokens/piece=68.8595\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11000 obj=8.9301 num_tokens=757496 num_tokens/piece=68.8633\ntrainer_interface.cc(687) LOG(INFO) Saving model: bpe_fr.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe_fr.vocab\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#Model loading\nsp_en = sentencepiece.SentencePieceProcessor(model_file='bpe_en.model')\nsp_fr = sentencepiece.SentencePieceProcessor(model_file='bpe_fr.model')\n\nstart_id = sp_fr.piece_to_id(\"<start>\")\nend_id = sp_fr.piece_to_id(\"<end>\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:37:45.756465Z","iopub.execute_input":"2025-05-04T19:37:45.756701Z","iopub.status.idle":"2025-05-04T19:37:45.797289Z","shell.execute_reply.started":"2025-05-04T19:37:45.756684Z","shell.execute_reply":"2025-05-04T19:37:45.796650Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#Converting the text into list of strings\nen_sentences = en_data_sample['sentence'].astype(str).tolist()\nfr_sentences = fr_data_sample['sentence'].astype(str).tolist()\n#Tokenization\n#Encode + Add special tokens\nen_tokenized = [sp_en.encode(s, out_type=int) for s in en_sentences]\nfr_tokenized = [[start_id] + sp_fr.encode(s, out_type=int) + [end_id] for s in fr_sentences]\n\n#Pad sequences\nmax_len = 30\nen_padded = pad_sequences(en_tokenized, maxlen=max_len, padding='post')\nfr_padded = pad_sequences(fr_tokenized, maxlen=max_len+2, padding='post')  # +2 for <start>, <end>\n\n#Shift for decoder input/target\ndecoder_input = fr_padded[:, :-1] \ndecoder_target = fr_padded[:, 1:] \nvocab_size_en = sp_en.get_piece_size()\nvocab_size_fr = sp_fr.get_piece_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T20:51:58.979065Z","iopub.execute_input":"2025-05-04T20:51:58.979324Z","iopub.status.idle":"2025-05-04T20:52:36.757145Z","shell.execute_reply.started":"2025-05-04T20:51:58.979305Z","shell.execute_reply":"2025-05-04T20:52:36.756344Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#Encoder Part\nembedding_dim = 100 #Hyper paramter to be tuned if needed\nlstm_units = 256 #Memory cells -> Neurons\n\nencoder_inputs = Input(shape=(None,))\nenc_emb = Embedding(vocab_size_en, embedding_dim)(encoder_inputs)\nencoder_lstm, state_h, state_c = LSTM(lstm_units, return_state=True,use_cudnn=True)(enc_emb)\n\nencoder_states = [state_h, state_c]\n\ndecoder_input_layer = Input(shape=(None,), name='decoder_input')\ndecoder_embed = Embedding(input_dim=vocab_size_fr,\n                          output_dim=embedding_dim,\n                          mask_zero=True,\n                          name='decoder_embedding')(decoder_input_layer)\n\ndecoder_outputs, _, _ = LSTM(units=lstm_units,\n                             return_sequences=True,\n                             return_state=True,\n                             name='decoder_lstm',\n                             use_cudnn=True)(decoder_embed, initial_state=encoder_states)\n\ndecoder_outputs = Dense(vocab_size_fr, activation='softmax')(decoder_outputs)\n\nmodel = Model(inputs=[encoder_inputs, decoder_input_layer], outputs=decoder_outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:20.859684Z","iopub.execute_input":"2025-05-04T19:38:20.859998Z","iopub.status.idle":"2025-05-04T19:38:23.334144Z","shell.execute_reply.started":"2025-05-04T19:38:20.859974Z","shell.execute_reply":"2025-05-04T19:38:23.333353Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1746387502.014913      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Summary\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:23.336371Z","iopub.execute_input":"2025-05-04T19:38:23.336586Z","iopub.status.idle":"2025-05-04T19:38:23.362333Z","shell.execute_reply.started":"2025-05-04T19:38:23.336569Z","shell.execute_reply":"2025-05-04T19:38:23.361639Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)      │      \u001b[38;5;34m1,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)      │      \u001b[38;5;34m1,000,000\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m365,568\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│                           │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),    │        \u001b[38;5;34m365,568\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)    │      \u001b[38;5;34m2,570,000\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,301,136\u001b[0m (20.22 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,301,136</span> (20.22 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,301,136\u001b[0m (20.22 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,301,136</span> (20.22 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"#Early stopping on validation loss\nes = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:23.362973Z","iopub.execute_input":"2025-05-04T19:38:23.363164Z","iopub.status.idle":"2025-05-04T19:38:23.366971Z","shell.execute_reply.started":"2025-05-04T19:38:23.363149Z","shell.execute_reply":"2025-05-04T19:38:23.366092Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Real training data\ndecoder_input_data = fr_padded[:, :-1]\ndecoder_target_data = fr_padded[:, 1:]\nhistory = model.fit(\n        [en_padded, decoder_input_data],                  # encoder input and decoder input data\n        decoder_target_data[..., np.newaxis],             # labels\n        batch_size=32,\n        epochs=10,\n        validation_split=0.1,\n        callbacks=[es],\n        verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:23.367613Z","iopub.execute_input":"2025-05-04T19:38:23.367828Z","iopub.status.idle":"2025-05-04T20:48:19.953574Z","shell.execute_reply.started":"2025-05-04T19:38:23.367808Z","shell.execute_reply":"2025-05-04T20:48:19.952989Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746387508.124728     793 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 30ms/step - accuracy: 0.2263 - loss: 4.4749 - val_accuracy: 0.2789 - val_loss: 4.8321\nEpoch 2/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 30ms/step - accuracy: 0.3372 - loss: 3.3285 - val_accuracy: 0.3032 - val_loss: 4.5770\nEpoch 3/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 30ms/step - accuracy: 0.3654 - loss: 2.9914 - val_accuracy: 0.3195 - val_loss: 4.4060\nEpoch 4/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 30ms/step - accuracy: 0.3824 - loss: 2.8089 - val_accuracy: 0.3322 - val_loss: 4.3208\nEpoch 5/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 30ms/step - accuracy: 0.3930 - loss: 2.6845 - val_accuracy: 0.3357 - val_loss: 4.2026\nEpoch 6/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 30ms/step - accuracy: 0.3996 - loss: 2.5930 - val_accuracy: 0.3397 - val_loss: 4.1496\nEpoch 7/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 30ms/step - accuracy: 0.4031 - loss: 2.5218 - val_accuracy: 0.3477 - val_loss: 4.1202\nEpoch 8/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 30ms/step - accuracy: 0.4079 - loss: 2.4637 - val_accuracy: 0.3465 - val_loss: 4.1075\nEpoch 9/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 30ms/step - accuracy: 0.4118 - loss: 2.4189 - val_accuracy: 0.3474 - val_loss: 4.0969\nEpoch 10/10\n\u001b[1m14063/14063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 30ms/step - accuracy: 0.4170 - loss: 2.3793 - val_accuracy: 0.3496 - val_loss: 4.0727\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"no_samples = 5000\nbleu = 0\nsmooth_fn = SmoothingFunction().method1\n\nfor i in range(no_samples):\n    x1 = model.predict([ en_padded[i+6:i+7],decoder_input_data[i+6:i+7] ],verbose = 0)\n    pred_ids = np.argmax(x1[0], axis=-1)  # shape: (sequence_len,)\n\n    # Decode the token ID sequences to text\n    reference_ids = decoder_target_data[i+6].tolist()\n    reference_text = sp_fr.decode_ids(reference_ids)\n    predicted_text = sp_fr.decode_ids(pred_ids.tolist())\n\n    # Tokenize the decoded text for BLEU\n    reference_tokens = reference_text.strip().split()\n    predicted_tokens = predicted_text.strip().split()\n\n    # Compute BLEU score\n    bleu += sentence_bleu([reference_tokens], predicted_tokens,smoothing_function = smooth_fn)\nprint(\"Average BLEU Score:\", bleu/no_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:05:04.341534Z","iopub.execute_input":"2025-05-04T21:05:04.341793Z","iopub.status.idle":"2025-05-04T21:10:52.201561Z","shell.execute_reply.started":"2025-05-04T21:05:04.341776Z","shell.execute_reply":"2025-05-04T21:10:52.200799Z"}},"outputs":[{"name":"stdout","text":"Average BLEU Score: 0.07070826352751303\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}